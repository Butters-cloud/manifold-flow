{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Gaussian experiment (2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import logging\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from manifold_flow import transforms, utils, distributions, training\n",
    "from manifold_flow.flows import Flow, ManifoldFlow\n",
    "from manifold_flow import nn as nn_\n",
    "from experiments.simulators.spherical_simulator import SphericalGaussianSimulator\n",
    "from experiments.utils import vector_transforms\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)-5.5s %(name)-30.30s %(levelname)-7.7s %(message)s\",\n",
    "    datefmt=\"%H:%M\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"experiments\" not in key and \"manifold_flow\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "n_train = 100000\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = SphericalGaussianSimulator(latent_dim=1, data_dim=2, epsilon=epsilon)\n",
    "x = simulator.sample(n_train)\n",
    "x_tensor = torch.from_numpy(x)\n",
    "train_dataset = TensorDataset(x_tensor, x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:50 experiments.simulators.spheric DEBUG   Evaluating true log density for x = [-1.4 -1.4]\n",
      "12:50 experiments.simulators.spheric DEBUG   Latent variables: z_phi = [3.92699082], z_eps = [0.97989899]\n",
      "../../experiments/simulators/spherical_simulator.py:108: RuntimeWarning: divide by zero encountered in log\n",
      "  logp_eps = np.log(norm(loc=0.0, scale=self._epsilon).pdf(z_eps))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = 200\n",
    "x_range = np.linspace(-1.4,1.4,res)\n",
    "y_range = np.linspace(-1.4,1.4,res)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "x_grid = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "logp_grid = simulator.log_density(x_grid, precise=True).reshape((res, res))\n",
    "logp_grid[~np.isfinite(logp_grid)] = -1000000.\n",
    "\n",
    "zmin, zmax = np.max(logp_grid) - 10, np.max(logp_grid)\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = plt.gca()\n",
    "\n",
    "pcm = plt.imshow(\n",
    "    np.clip(logp_grid, zmin, zmax),\n",
    "    extent=(-1.4,1.4,-1.4,1.4),\n",
    "    origin=\"lower\",\n",
    "    cmap=\"viridis\", norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "    interpolation='nearest'\n",
    ")\n",
    "cb = plt.colorbar(pcm, extend=\"both\")\n",
    "plt.scatter(x[::50,0], x[::50,1], s=2., c=\"black\", alpha=1.)\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "cb.set_label(\"$\\log \\; p(x)$\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"spherical_gaussian_2d_data.pdf\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000930850098504"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_size = (x_range[1] - x_range[0]) * (y_range[1] - y_range[0])\n",
    "\n",
    "np.sum(np.exp(logp_grid) * pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:50 manifold_flow.transforms.proje DEBUG   Set up projection from vector with dimension 2 to vector with dimension 1\n",
      "12:50 manifold_flow.flows.base       DEBUG   Created standard flow with 0.0 M parameters (0.0 M trainable) with an estimated size of 0.0 <B\n",
      "12:50 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "12:50 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "12:50 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "12:50 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "12:50 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "12:50 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "12:50 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "12:50 manifold_flow.training.trainer DEBUG   Training epoch 1 / 4\n",
      "12:50 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "12:50 manifold_flow.training.trainer INFO    Epoch   1: train loss  0.69685 (mse:  0.070)\n",
      "12:50 manifold_flow.training.trainer INFO               val. loss   0.40174 (mse:  0.040)\n",
      "12:50 manifold_flow.training.trainer DEBUG   Training epoch 2 / 4\n",
      "12:50 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "12:50 manifold_flow.training.trainer INFO    Epoch   2: train loss  0.40524 (mse:  0.041)\n",
      "12:50 manifold_flow.training.trainer INFO               val. loss   0.39372 (mse:  0.039)\n",
      "12:50 manifold_flow.training.trainer DEBUG   Training epoch 3 / 4\n",
      "12:50 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "12:50 manifold_flow.training.trainer INFO    Epoch   3: train loss  0.39566 (mse:  0.040)\n",
      "12:50 manifold_flow.training.trainer INFO               val. loss   0.38788 (mse:  0.039)\n",
      "12:50 manifold_flow.training.trainer DEBUG   Training epoch 4 / 4\n",
      "12:50 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "12:50 manifold_flow.training.trainer INFO    Epoch   4: train loss  0.38916 (mse:  0.039)\n",
      "12:50 manifold_flow.training.trainer INFO               val. loss   0.38115 (mse:  0.038)\n",
      "12:50 manifold_flow.training.trainer INFO    Early stopping did not improve performance\n",
      "12:50 manifold_flow.training.trainer DEBUG   Training finished\n",
      "12:50 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "12:50 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "12:50 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "12:50 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "12:50 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "12:50 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "12:50 manifold_flow.training.trainer DEBUG   Training epoch 1 / 4\n",
      "12:50 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "12:54 manifold_flow.training.trainer INFO    Epoch   1: train loss  0.41373 (mse:  0.039, nll:  2.773)\n",
      "12:54 manifold_flow.training.trainer INFO               val. loss   0.40604 (mse:  0.038, nll:  2.641)\n",
      "12:54 manifold_flow.training.trainer DEBUG   Training epoch 2 / 4\n",
      "12:54 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "12:58 manifold_flow.training.trainer INFO    Epoch   2: train loss  0.40494 (mse:  0.038, nll:  2.549)\n",
      "12:58 manifold_flow.training.trainer INFO               val. loss   0.40327 (mse:  0.038, nll:  2.464)\n",
      "12:58 manifold_flow.training.trainer DEBUG   Training epoch 3 / 4\n",
      "12:58 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "13:02 manifold_flow.training.trainer INFO    Epoch   3: train loss  0.39850 (mse:  0.037, nll:  2.440)\n",
      "13:02 manifold_flow.training.trainer INFO               val. loss   0.39268 (mse:  0.037, nll:  2.402)\n",
      "13:02 manifold_flow.training.trainer DEBUG   Training epoch 4 / 4\n",
      "13:02 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "13:06 manifold_flow.training.trainer INFO    Epoch   4: train loss  0.39318 (mse:  0.037, nll:  2.393)\n",
      "13:06 manifold_flow.training.trainer INFO               val. loss   0.38967 (mse:  0.037, nll:  2.371)\n",
      "13:06 manifold_flow.training.trainer INFO    Early stopping did not improve performance\n",
      "13:06 manifold_flow.training.trainer DEBUG   Training finished\n",
      "13:06 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "13:06 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "13:06 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "13:06 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "13:06 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "13:06 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "13:06 manifold_flow.training.trainer DEBUG   Training epoch 1 / 4\n",
      "13:06 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "13:10 manifold_flow.training.trainer INFO    Epoch   1: train loss  2.31613 (mse:  0.037, nll:  2.316)\n",
      "13:10 manifold_flow.training.trainer INFO               val. loss   2.28478 (mse:  0.036, nll:  2.285)\n",
      "13:10 manifold_flow.training.trainer DEBUG   Training epoch 2 / 4\n",
      "13:10 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "13:15 manifold_flow.training.trainer INFO    Epoch   2: train loss  2.23779 (mse:  0.037, nll:  2.238)\n",
      "13:15 manifold_flow.training.trainer INFO               val. loss   2.22471 (mse:  0.036, nll:  2.225)\n",
      "13:15 manifold_flow.training.trainer DEBUG   Training epoch 3 / 4\n",
      "13:15 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "13:19 manifold_flow.training.trainer INFO    Epoch   3: train loss  2.19163 (mse:  0.037, nll:  2.192)\n",
      "13:19 manifold_flow.training.trainer INFO               val. loss   2.19172 (mse:  0.036, nll:  2.192)\n",
      "13:19 manifold_flow.training.trainer DEBUG   Training epoch 4 / 4\n",
      "13:19 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "13:23 manifold_flow.training.trainer INFO    Epoch   4: train loss  2.16846 (mse:  0.037, nll:  2.168)\n",
      "13:23 manifold_flow.training.trainer INFO               val. loss   2.17772 (mse:  0.036, nll:  2.178)\n",
      "13:23 manifold_flow.training.trainer INFO    Early stopping did not improve performance\n",
      "13:23 manifold_flow.training.trainer DEBUG   Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2.31613124, 2.23778944, 2.19162874, 2.16845725]),\n",
       " array([2.28478424, 2.2247114 , 2.19172427, 2.17771641]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_transform = vector_transforms.create_transform(\n",
    "    2, 3,\n",
    "    linear_transform_type=\"permutation\",\n",
    "    base_transform_type=\"affine-coupling\",\n",
    "    context_features=None,\n",
    "    hidden_features=20,\n",
    "    num_transform_blocks=1,\n",
    "    resnet_transform=False,\n",
    ")\n",
    "inner_transform=transforms.ConditionalAffineScalarTransform(features=1)\n",
    "\n",
    "mf = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    inner_transform=inner_transform,\n",
    "    outer_transform=outer_transform\n",
    ")\n",
    "\n",
    "trainer = training.trainer.ManifoldFlowTrainer(mf)\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.mse],\n",
    "    loss_weights=[10.],\n",
    "    epochs=epochs // 3,\n",
    "    forward_kwargs={\"mode\":\"projection\"}\n",
    ")\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.mse, training.losses.nll],\n",
    "    loss_weights=[10., 0.01],\n",
    "    epochs=epochs // 3,\n",
    "    forward_kwargs={\"mode\":\"mf\"}\n",
    ")\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.mse, training.losses.nll],\n",
    "    loss_weights=[0., 1.],\n",
    "    epochs=epochs // 3,\n",
    "    parameters=mf.inner_transform.parameters(),\n",
    "    forward_kwargs={\"mode\":\"mf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:23 manifold_flow.transforms.proje DEBUG   Set up projection from vector with dimension 2 to vector with dimension 1\n",
      "13:23 manifold_flow.flows.base       DEBUG   Created standard flow with 2.0 M parameters (2.0 M trainable) with an estimated size of 7.9 <B\n",
      "13:23 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "13:23 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "13:23 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "13:23 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "13:23 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "13:23 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "13:23 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "13:23 manifold_flow.training.trainer DEBUG   Training epoch 1 / 12\n",
      "13:23 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "13:24 manifold_flow.training.trainer INFO    Epoch   1: train loss 5934.88216 (nll: 5934.882)\n",
      "13:24 manifold_flow.training.trainer INFO               val. loss  16.21244 (nll: 16.212)\n",
      "13:24 manifold_flow.training.trainer DEBUG   Training epoch 2 / 12\n",
      "13:24 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "13:25 manifold_flow.training.trainer INFO    Epoch   2: train loss 25.18586 (nll: 25.186)\n",
      "13:25 manifold_flow.training.trainer INFO               val. loss  47.51821 (nll: 47.518)\n",
      "13:25 manifold_flow.training.trainer DEBUG   Training epoch 3 / 12\n",
      "13:25 manifold_flow.training.trainer DEBUG     Learning rate: 0.0009829629131445341\n",
      "13:26 manifold_flow.training.trainer INFO    Epoch   3: train loss 49.92035 (nll: 49.920)\n",
      "13:26 manifold_flow.training.trainer INFO               val. loss   5.98934 (nll:  5.989)\n",
      "13:26 manifold_flow.training.trainer DEBUG   Training epoch 4 / 12\n",
      "13:26 manifold_flow.training.trainer DEBUG     Learning rate: 0.0009330127018922195\n",
      "14:05 manifold_flow.training.trainer INFO    Epoch   4: train loss 63.62024 (nll: 63.620)\n",
      "14:05 manifold_flow.training.trainer INFO               val. loss  110.17488 (nll: 110.175)\n",
      "14:05 manifold_flow.training.trainer DEBUG   Training epoch 5 / 12\n",
      "14:05 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "14:06 manifold_flow.training.trainer INFO    Epoch   5: train loss 77.25136 (nll: 77.251)\n",
      "14:06 manifold_flow.training.trainer INFO               val. loss  70.99300 (nll: 70.993)\n",
      "14:06 manifold_flow.training.trainer DEBUG   Training epoch 6 / 12\n",
      "14:06 manifold_flow.training.trainer DEBUG     Learning rate: 0.00075\n",
      "14:08 manifold_flow.training.trainer INFO    Epoch   6: train loss 57.86612 (nll: 57.866)\n",
      "14:08 manifold_flow.training.trainer INFO               val. loss  83.30112 (nll: 83.301)\n",
      "14:08 manifold_flow.training.trainer DEBUG   Training epoch 7 / 12\n",
      "14:08 manifold_flow.training.trainer DEBUG     Learning rate: 0.0006294095225512603\n",
      "14:09 manifold_flow.training.trainer INFO    Epoch   7: train loss 40.12511 (nll: 40.125)\n",
      "14:09 manifold_flow.training.trainer INFO               val. loss  17.74026 (nll: 17.740)\n",
      "14:09 manifold_flow.training.trainer DEBUG   Training epoch 8 / 12\n",
      "14:09 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "14:11 manifold_flow.training.trainer INFO    Epoch   8: train loss 29.34164 (nll: 29.342)\n",
      "14:11 manifold_flow.training.trainer INFO               val. loss  21.77191 (nll: 21.772)\n",
      "14:11 manifold_flow.training.trainer DEBUG   Training epoch 9 / 12\n",
      "14:11 manifold_flow.training.trainer DEBUG     Learning rate: 0.0003705904774487397\n",
      "14:12 manifold_flow.training.trainer INFO    Epoch   9: train loss 13.57379 (nll: 13.574)\n",
      "14:12 manifold_flow.training.trainer INFO               val. loss   9.06427 (nll:  9.064)\n",
      "14:12 manifold_flow.training.trainer DEBUG   Training epoch 10 / 12\n",
      "14:12 manifold_flow.training.trainer DEBUG     Learning rate: 0.0002500000000000001\n",
      "14:13 manifold_flow.training.trainer INFO    Epoch  10: train loss  6.17261 (nll:  6.173)\n",
      "14:13 manifold_flow.training.trainer INFO               val. loss   3.61291 (nll:  3.613)\n",
      "14:13 manifold_flow.training.trainer DEBUG   Training epoch 11 / 12\n",
      "14:13 manifold_flow.training.trainer DEBUG     Learning rate: 0.00014644660940672628\n",
      "14:15 manifold_flow.training.trainer INFO    Epoch  11: train loss  1.80985 (nll:  1.810)\n",
      "14:15 manifold_flow.training.trainer INFO               val. loss   1.98478 (nll:  1.985)\n",
      "14:15 manifold_flow.training.trainer DEBUG   Training epoch 12 / 12\n",
      "14:15 manifold_flow.training.trainer DEBUG     Learning rate: 6.698729810778065e-05\n",
      "14:16 manifold_flow.training.trainer INFO    Epoch  12: train loss -0.19272 (nll: -0.193)\n",
      "14:16 manifold_flow.training.trainer INFO               val. loss  -0.84813 (nll: -0.848)\n",
      "14:16 manifold_flow.training.trainer INFO    Early stopping did not improve performance\n",
      "14:16 manifold_flow.training.trainer DEBUG   Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 5.93488216e+03,  2.51858625e+01,  4.99203524e+01,  6.36202417e+01,\n",
       "         7.72513592e+01,  5.78661190e+01,  4.01251104e+01,  2.93416373e+01,\n",
       "         1.35737875e+01,  6.17260819e+00,  1.80984950e+00, -1.92719718e-01]),\n",
       " array([ 16.21243673,  47.51820895,   5.98933511, 110.17487866,\n",
       "         70.99299747,  83.30112271,  17.74026494,  21.77191319,\n",
       "          9.06426881,   3.61291135,   1.98477712,  -0.84812755]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_transform = vector_transforms.create_transform(\n",
    "    2, 5,\n",
    "    linear_transform_type=\"permutation\",\n",
    "    base_transform_type=\"affine-coupling\",\n",
    "    context_features=None\n",
    ")\n",
    "inner_transform=transforms.ConditionalAffineScalarTransform(features=1)\n",
    "\n",
    "pie = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    outer_transform=outer_transform,\n",
    "    inner_transform=inner_transform,\n",
    ")\n",
    "\n",
    "trainer = training.trainer.ManifoldFlowTrainer(pie)\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.nll],\n",
    "    loss_weights=[1.],\n",
    "    epochs=epochs,\n",
    "    forward_kwargs={\"mode\":\"pie\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice of PIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:16 manifold_flow.transforms.proje DEBUG   Set up projection from vector with dimension 2 to vector with dimension 1\n",
      "14:16 manifold_flow.flows.base       DEBUG   Created standard flow with 2.0 M parameters (2.0 M trainable) with an estimated size of 7.9 <B\n",
      "14:16 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "14:16 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "14:16 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "14:16 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "14:16 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "14:16 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "14:16 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "14:16 manifold_flow.training.trainer DEBUG   Training epoch 1 / 4\n",
      "14:16 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:18 manifold_flow.training.trainer INFO    Epoch   1: train loss  0.10325 (mse:  0.010)\n",
      "14:18 manifold_flow.training.trainer INFO               val. loss   0.02190 (mse:  0.002)\n",
      "14:18 manifold_flow.training.trainer DEBUG   Training epoch 2 / 4\n",
      "14:18 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:20 manifold_flow.training.trainer INFO    Epoch   2: train loss  0.05019 (mse:  0.005)\n",
      "14:20 manifold_flow.training.trainer INFO               val. loss   0.01275 (mse:  0.001)\n",
      "14:20 manifold_flow.training.trainer DEBUG   Training epoch 3 / 4\n",
      "14:20 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "14:21 manifold_flow.training.trainer INFO    Epoch   3: train loss  0.03939 (mse:  0.004)\n",
      "14:21 manifold_flow.training.trainer INFO               val. loss   0.01158 (mse:  0.001)\n",
      "14:21 manifold_flow.training.trainer DEBUG   Training epoch 4 / 4\n",
      "14:21 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "14:23 manifold_flow.training.trainer INFO    Epoch   4: train loss  0.01126 (mse:  0.001)\n",
      "14:23 manifold_flow.training.trainer INFO               val. loss   0.02190 (mse:  0.002)\n",
      "14:23 manifold_flow.training.trainer INFO    Early stopping after epoch 3, with loss  0.01158 compared to final loss  0.02190\n",
      "14:23 manifold_flow.training.trainer DEBUG   Training finished\n",
      "14:23 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "14:23 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "14:23 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "14:23 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "14:23 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "14:23 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "14:23 manifold_flow.training.trainer DEBUG   Training epoch 1 / 4\n",
      "14:23 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:24 manifold_flow.training.trainer INFO    Epoch   1: train loss  0.00350 (mse:  0.004, nll: -3.934)\n",
      "14:24 manifold_flow.training.trainer INFO               val. loss  -0.02714 (mse:  0.002, nll: -4.333)\n",
      "14:24 manifold_flow.training.trainer DEBUG   Training epoch 2 / 4\n",
      "14:24 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:26 manifold_flow.training.trainer INFO    Epoch   2: train loss -0.00024 (mse:  0.004, nll: -4.400)\n",
      "14:26 manifold_flow.training.trainer INFO               val. loss  -0.03612 (mse:  0.001, nll: -4.382)\n",
      "14:26 manifold_flow.training.trainer DEBUG   Training epoch 3 / 4\n",
      "14:26 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "14:27 manifold_flow.training.trainer INFO    Epoch   3: train loss -0.02148 (mse:  0.002, nll: -4.456)\n",
      "14:27 manifold_flow.training.trainer INFO               val. loss  -0.03088 (mse:  0.002, nll: -4.608)\n",
      "14:27 manifold_flow.training.trainer DEBUG   Training epoch 4 / 4\n",
      "14:27 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "14:29 manifold_flow.training.trainer INFO    Epoch   4: train loss -0.02933 (mse:  0.002, nll: -4.524)\n",
      "14:29 manifold_flow.training.trainer INFO               val. loss  -0.03824 (mse:  0.001, nll: -4.570)\n",
      "14:29 manifold_flow.training.trainer INFO    Early stopping did not improve performance\n",
      "14:29 manifold_flow.training.trainer DEBUG   Training finished\n",
      "14:29 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "14:29 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "14:29 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "14:29 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "14:29 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "14:29 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "14:29 manifold_flow.training.trainer DEBUG   Training epoch 1 / 4\n",
      "14:29 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:30 manifold_flow.training.trainer INFO    Epoch   1: train loss -4.56846 (mse:  0.001, nll: -4.568)\n",
      "14:30 manifold_flow.training.trainer INFO               val. loss  -4.56843 (mse:  0.001, nll: -4.568)\n",
      "14:30 manifold_flow.training.trainer DEBUG   Training epoch 2 / 4\n",
      "14:30 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:43 manifold_flow.training.trainer INFO    Epoch   2: train loss -4.56856 (mse:  0.001, nll: -4.569)\n",
      "14:43 manifold_flow.training.trainer INFO               val. loss  -4.56843 (mse:  0.001, nll: -4.568)\n",
      "14:43 manifold_flow.training.trainer DEBUG   Training epoch 3 / 4\n",
      "14:43 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "14:45 manifold_flow.training.trainer INFO    Epoch   3: train loss -4.56879 (mse:  0.001, nll: -4.569)\n",
      "14:45 manifold_flow.training.trainer INFO               val. loss  -4.56843 (mse:  0.001, nll: -4.568)\n",
      "14:45 manifold_flow.training.trainer DEBUG   Training epoch 4 / 4\n",
      "14:45 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "14:46 manifold_flow.training.trainer INFO    Epoch   4: train loss -4.56862 (mse:  0.001, nll: -4.569)\n",
      "14:46 manifold_flow.training.trainer INFO               val. loss  -4.56843 (mse:  0.001, nll: -4.568)\n",
      "14:46 manifold_flow.training.trainer INFO    Early stopping after epoch 2, with loss -4.56843 compared to final loss -4.56843\n",
      "14:46 manifold_flow.training.trainer DEBUG   Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-4.56845531, -4.56855519, -4.56878636, -4.56861585]),\n",
       " array([-4.56842957, -4.56842958, -4.56842954, -4.56842957]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_transform = vector_transforms.create_transform(\n",
    "    2, 5,\n",
    "    linear_transform_type=\"permutation\",\n",
    "    base_transform_type=\"affine-coupling\",\n",
    "    context_features=None\n",
    ")\n",
    "inner_transform=transforms.ConditionalAffineScalarTransform(features=1)\n",
    "\n",
    "slice_of_pie = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    outer_transform=outer_transform,\n",
    "    inner_transform=inner_transform,\n",
    ")\n",
    "\n",
    "trainer = training.trainer.ManifoldFlowTrainer(slice_of_pie)\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.mse],\n",
    "    loss_weights=[10.],\n",
    "    epochs=epochs // 3,\n",
    "    forward_kwargs={\"mode\": \"projection\"}\n",
    ")\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.mse, training.losses.nll],\n",
    "    loss_weights=[10., 0.01],\n",
    "    epochs=epochs // 3,\n",
    "    forward_kwargs={\"mode\": \"slice\"}\n",
    ")\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.mse, training.losses.nll],\n",
    "    loss_weights=[0., 1.],\n",
    "    epochs=epochs // 3,\n",
    "    parameters=mf.inner_transform.parameters(),\n",
    "    forward_kwargs={\"mode\": \"slice\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:46 manifold_flow.flows.base       DEBUG   Created standard flow with 2.0 M parameters (2.0 M trainable) with an estimated size of 7.9 <B\n",
      "14:46 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "14:46 manifold_flow.training.trainer DEBUG   Initialising training data\n",
      "14:46 manifold_flow.training.trainer DEBUG   Setting up optimizer\n",
      "14:46 manifold_flow.training.trainer DEBUG   Setting up LR scheduler\n",
      "14:46 manifold_flow.training.trainer DEBUG   Using early stopping with infinite patience\n",
      "14:46 manifold_flow.training.trainer DEBUG   Will print training progress every 1 epochs\n",
      "14:46 manifold_flow.training.trainer DEBUG   Beginning main training loop\n",
      "14:46 manifold_flow.training.trainer DEBUG   Training epoch 1 / 12\n",
      "14:46 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:47 manifold_flow.training.trainer INFO    Epoch   1: train loss  1.90910 (nll:  1.909)\n",
      "14:47 manifold_flow.training.trainer INFO               val. loss   1.87421 (nll:  1.874)\n",
      "14:47 manifold_flow.training.trainer DEBUG   Training epoch 2 / 12\n",
      "14:47 manifold_flow.training.trainer DEBUG     Learning rate: 0.001\n",
      "14:49 manifold_flow.training.trainer INFO    Epoch   2: train loss  1.88287 (nll:  1.883)\n",
      "14:49 manifold_flow.training.trainer INFO               val. loss   1.86490 (nll:  1.865)\n",
      "14:49 manifold_flow.training.trainer DEBUG   Training epoch 3 / 12\n",
      "14:49 manifold_flow.training.trainer DEBUG     Learning rate: 0.0009829629131445341\n",
      "14:50 manifold_flow.training.trainer INFO    Epoch   3: train loss  1.87316 (nll:  1.873)\n",
      "14:50 manifold_flow.training.trainer INFO               val. loss   1.87583 (nll:  1.876)\n",
      "14:50 manifold_flow.training.trainer DEBUG   Training epoch 4 / 12\n",
      "14:50 manifold_flow.training.trainer DEBUG     Learning rate: 0.0009330127018922195\n",
      "14:52 manifold_flow.training.trainer INFO    Epoch   4: train loss  1.87302 (nll:  1.873)\n",
      "14:52 manifold_flow.training.trainer INFO               val. loss   1.88761 (nll:  1.888)\n",
      "14:52 manifold_flow.training.trainer DEBUG   Training epoch 5 / 12\n",
      "14:52 manifold_flow.training.trainer DEBUG     Learning rate: 0.0008535533905932737\n",
      "14:53 manifold_flow.training.trainer INFO    Epoch   5: train loss  1.86922 (nll:  1.869)\n",
      "14:53 manifold_flow.training.trainer INFO               val. loss   1.87627 (nll:  1.876)\n",
      "14:53 manifold_flow.training.trainer DEBUG   Training epoch 6 / 12\n",
      "14:53 manifold_flow.training.trainer DEBUG     Learning rate: 0.00075\n",
      "14:55 manifold_flow.training.trainer INFO    Epoch   6: train loss  1.86640 (nll:  1.866)\n",
      "14:55 manifold_flow.training.trainer INFO               val. loss   1.85601 (nll:  1.856)\n",
      "14:55 manifold_flow.training.trainer DEBUG   Training epoch 7 / 12\n",
      "14:55 manifold_flow.training.trainer DEBUG     Learning rate: 0.0006294095225512603\n",
      "14:56 manifold_flow.training.trainer INFO    Epoch   7: train loss  1.86116 (nll:  1.861)\n",
      "14:56 manifold_flow.training.trainer INFO               val. loss   1.85796 (nll:  1.858)\n",
      "14:56 manifold_flow.training.trainer DEBUG   Training epoch 8 / 12\n",
      "14:56 manifold_flow.training.trainer DEBUG     Learning rate: 0.0005\n",
      "14:58 manifold_flow.training.trainer INFO    Epoch   8: train loss  1.86055 (nll:  1.861)\n",
      "14:58 manifold_flow.training.trainer INFO               val. loss   1.87289 (nll:  1.873)\n",
      "14:58 manifold_flow.training.trainer DEBUG   Training epoch 9 / 12\n",
      "14:58 manifold_flow.training.trainer DEBUG     Learning rate: 0.0003705904774487397\n",
      "14:59 manifold_flow.training.trainer INFO    Epoch   9: train loss  1.85803 (nll:  1.858)\n",
      "14:59 manifold_flow.training.trainer INFO               val. loss   1.85712 (nll:  1.857)\n",
      "14:59 manifold_flow.training.trainer DEBUG   Training epoch 10 / 12\n",
      "14:59 manifold_flow.training.trainer DEBUG     Learning rate: 0.0002500000000000001\n",
      "15:01 manifold_flow.training.trainer INFO    Epoch  10: train loss  1.85664 (nll:  1.857)\n",
      "15:01 manifold_flow.training.trainer INFO               val. loss   1.85366 (nll:  1.854)\n",
      "15:01 manifold_flow.training.trainer DEBUG   Training epoch 11 / 12\n",
      "15:01 manifold_flow.training.trainer DEBUG     Learning rate: 0.00014644660940672628\n"
     ]
    }
   ],
   "source": [
    "transform = vector_transforms.create_transform(\n",
    "    2, 5,\n",
    "    linear_transform_type=\"permutation\",\n",
    "    base_transform_type=\"affine-coupling\",\n",
    "    context_features=None\n",
    ")\n",
    "sf = Flow(\n",
    "    data_dim=2,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainer = training.trainer.ManifoldFlowTrainer(sf)\n",
    "trainer.train(\n",
    "    train_dataset,\n",
    "    [training.losses.nll],\n",
    "    loss_weights=[1.],\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generative performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen_sf = sf.sample(n=1000).detach().numpy()\n",
    "x_gen_mf = mf.sample(n=1000).detach().numpy()\n",
    "x_gen_pie = pie.sample(n=1000).detach().numpy()\n",
    "x_gen_pie_full = pie.sample(n=1000, sample_orthogonal=True).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance from manifold, true likelihood of generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_min = -1000\n",
    "\n",
    "logp_gen_sf = simulator.log_density(x_gen_sf)\n",
    "logp_gen_mf = simulator.log_density(x_gen_mf)\n",
    "logp_gen_pie = simulator.log_density(x_gen_pie)\n",
    "logp_gen_pie_full = simulator.log_density(x_gen_pie_full)\n",
    "\n",
    "logp_gen_sf[(~np.isfinite(logp_gen_sf)) + (logp_gen_sf<logp_min)] = logp_min\n",
    "logp_gen_mf[(~np.isfinite(logp_gen_mf)) + (logp_gen_mf<logp_min)] = logp_min\n",
    "logp_gen_pie[(~np.isfinite(logp_gen_pie)) + (logp_gen_pie<logp_min)] = logp_min\n",
    "logp_gen_pie_full[(~np.isfinite(logp_gen_pie_full)) + (logp_gen_pie_full<logp_min)] = logp_min\n",
    "\n",
    "mean_logp_gen_sf = np.mean(logp_gen_sf)\n",
    "mean_logp_gen_mf = np.mean(logp_gen_mf)\n",
    "mean_logp_gen_pie = np.mean(logp_gen_pie)\n",
    "mean_logp_gen_pie_full = np.mean(logp_gen_pie_full)\n",
    "\n",
    "distance_from_manifold_gen_sf = np.mean(np.abs(np.sum(x_gen_sf**2, axis=1)**0.5 - 1))\n",
    "distance_from_manifold_gen_mf = np.mean(np.abs(np.sum(x_gen_mf**2, axis=1)**0.5 - 1))\n",
    "distance_from_manifold_gen_pie = np.mean(np.abs(np.sum(x_gen_pie**2, axis=1)**0.5 - 1))\n",
    "distance_from_manifold_gen_pie_full = np.mean(np.abs(np.sum(x_gen_pie_full**2, axis=1)**0.5 - 1))\n",
    "\n",
    "print(\"Mean true log likelihood of samples generated from flows (higher is better):\")\n",
    "print(\"  Standard flow:      {:>6.1f}\".format(mean_logp_gen_sf))\n",
    "print(\"  PIE:                {:>6.1f}\".format(mean_logp_gen_pie))\n",
    "print(\"  PIE (sampling all): {:>6.1f}\".format(mean_logp_gen_pie_full))\n",
    "print(\"  Manifold flow:      {:>6.1f}\".format(mean_logp_gen_mf))\n",
    "\n",
    "print(\"Mean Euclidean distance between samples generated from flows and true manifold (lower is better):\")\n",
    "print(\"  Standard flow:      {:>6.2f}\".format(distance_from_manifold_gen_sf))\n",
    "print(\"  PIE:                {:>6.2f}\".format(distance_from_manifold_gen_pie))\n",
    "print(\"  PIE (sampling all): {:>6.2f}\".format(distance_from_manifold_gen_pie_full))\n",
    "print(\"  Manifold flow:      {:>6.2f}\".format(distance_from_manifold_gen_mf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 10\n",
    "res = 250\n",
    "x_range = np.linspace(-1.5,1.5,res)\n",
    "y_range = np.linspace(-1.5,1.5,res)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "x_grid = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "logp_grid = simulator.log_density(x_grid).reshape((res, res))\n",
    "logp_grid[~np.isfinite(logp_grid)] = -1000000.\n",
    "zmin, zmax = np.max(logp_grid) - 10., np.max(logp_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "pcm = plt.imshow(\n",
    "    np.clip(logp_grid, zmin, zmax),\n",
    "    extent=(-1.5,1.5,-1.5,1.5),\n",
    "    origin=\"lower\",\n",
    "    cmap=\"Greys\",\n",
    "    norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "    interpolation='nearest'\n",
    ")\n",
    "cb = plt.colorbar(pcm, extend=\"both\")\n",
    "\n",
    "plt.scatter(x_gen_sf[::skip,0], x_gen_sf[::skip,1], s=10., c=\"C0\", label=\"Standard flow\")\n",
    "plt.scatter(x_gen_pie[::skip,0], x_gen_pie[::skip,1], s=10., c=\"C1\", label=\"PIE\")\n",
    "plt.scatter(x_gen_pie_full[::skip,0], x_gen_pie_full[::skip,1], s=10., c=\"C2\", label=\"PIE (sampling all)\")\n",
    "plt.scatter(x_gen_mf[::skip,0], x_gen_mf[::skip,1], s=10., c=\"C3\", label=\"Manifold flow\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "cb.set_label(\"True log density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"spherical_gaussian_2d_generation.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learned likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate log likelihood on grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 100\n",
    "x_range = np.linspace(-1.5,1.5,res)\n",
    "y_range = np.linspace(-1.5,1.5,res)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "x_grid = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "x_grid_tensor = torch.FloatTensor(x_grid)\n",
    "\n",
    "learned_logp_grid_sf = sf.log_prob(x_grid_tensor).detach().numpy().reshape((res, res))\n",
    "learned_logp_grid_pie = pie.log_prob(x_grid_tensor, mode=\"pie\").detach().numpy().reshape((res, res))\n",
    "\n",
    "# learned_logp_grid_mf = mf.log_prob(x_grid_tensor, mode=\"mf\").detach().numpy().reshape((res, res))\n",
    "# learned_logp_grid_slice_of_pie = slice_of_pie.log_prob(x_grid_tensor, mode=\"slice\").detach().numpy().reshape((res, res))\n",
    "\n",
    "logp_grid_truth = simulator.log_density(x_grid).reshape((res, res))\n",
    "logp_grid_truth[~np.isfinite(logp_grid_truth)] = -1000000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate along manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_along_manifold(model, mode, zmin=-5., zmax=5., n_samples=100, epsilon=0.05):\n",
    "    # Sample\n",
    "    u = torch.linspace(zmin, zmax, n_samples)\n",
    "    x = model.sample(n=n_samples, u=u).detach().numpy()\n",
    "    \n",
    "    # Interpolate\n",
    "    x_interpol = [x[0]]\n",
    "    for x, x_prev in zip(x[:-1], x[1:]):\n",
    "        distance = np.linalg.norm(x-x_prev)\n",
    "        if distance > epsilon:\n",
    "            n_insert = int(distance / epsilon)\n",
    "            stops = np.linspace(0., 1., n_insert + 2)[1:-1]\n",
    "            for frac in np.linspace(0., 1., n_insert + 2)[1:-1]:\n",
    "                x_interpol.append(x_prev + stop * (x-x_prev))\n",
    "        x_interpol.append(x)\n",
    "    x_interpol.append(x)\n",
    "    x_interpol = np.array(x_interpol)\n",
    "\n",
    "    # Evaluate likelihood\n",
    "    log_probs = model.log_prob(torch.FloatTensor(x_interpol, mode=mode).detach().numpy()\n",
    "\n",
    "    # Return\n",
    "    return x_interpol, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_manifold_mf, learned_logp_manifold_mf = likelihood_along_manifold(mf, \"mf\")\n",
    "x_manifold_slice, learned_logp_manifold_slice = likelihood_along_manifold(mf, \"slice\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 500\n",
    "\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "\n",
    "for i, (label, logp, x) in enumerate(zip(\n",
    "    [\"True log density\", \"Standard flow log density\", \"PIE log density\",\n",
    "     \"Slice of PIE log density\", \"Manifold flow log density\"],\n",
    "    [logp_grid_truth, learned_logp_grid_sf, learned_logp_grid_pie,\n",
    "     learned_logp_manifold_slice, learned_logp_manifold_mf],\n",
    "    [None, None, None,\n",
    "     x_manifold_slice, x_manifold_mf]\n",
    "     )):\n",
    "    ax = plt.subplot(2,3,i+1)\n",
    "\n",
    "    zmin, zmax = np.max(logp) - 5., np.max(logp)\n",
    "\n",
    "    if x is None:\n",
    "        pcm = plt.imshow(\n",
    "            np.clip(logp, zmin, zmax),\n",
    "            extent=(-1.5, 1.5, -1.5, 1.5),\n",
    "            origin=\"lower\",\n",
    "            cmap=\"viridis\",\n",
    "            norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "            interpolation='nearest'\n",
    "        )\n",
    "        cb = plt.colorbar(pcm, extend=\"both\")\n",
    "        \n",
    "    else:\n",
    "        pcm = plt.scatter(\n",
    "            x[:,0], x[:,1],\n",
    "            c=np.clip(logp, zmin, zmax),\n",
    "            s=15.,\n",
    "            cmap=\"viridis\",\n",
    "            norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "        )\n",
    "\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    cb.set_label(label)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"spherical_gaussian_2d_log_prob.pdf\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot reconstruction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = SphericalGaussianSimulator(1,2,epsilon=4*epsilon).sample(100)\n",
    "x_in = torch.FloatTensor(x_in)\n",
    "x_out = mf(x_in)[0]\n",
    "x_in, x_out = x_in.detach().numpy(), x_out.detach().numpy()\n",
    "dx = x_out - x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "plt.scatter(x_in[:,0], x_in[:,1], s=15., c=\"C1\")\n",
    "plt.scatter(x_out[:,0], x_out[:,1], s=15., c=\"C0\")\n",
    "plt.quiver(\n",
    "    x_in[:,0], x_in[:,1], dx[:,0], dx[:,1],\n",
    "    angles='xy', scale_units='xy', scale=1., width=2.e-3, alpha=1.\n",
    ")\n",
    "\n",
    "plt.xlim(-1.5,1.5)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../figures/spherical_gaussian_2d_mf_reco.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 25\n",
    "x_range = np.linspace(-1.5,1.5,res)\n",
    "y_range = np.linspace(-1.5,1.5,res)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "x = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "x = torch.FloatTensor(x)\n",
    "x.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "u, h_manifold, h_orthogonal, log_det_outer, log_det_inner = model._encode(x)\n",
    "\n",
    "# Decode\n",
    "x_reco, _, _, inv_jacobian_outer = model._decode(u, mode=\"mf\")\n",
    "_, inv_log_det_inner, inv_log_det_outer, _ = model._decode(u, mode=\"slice\")\n",
    "\n",
    "# inv_jacobian_outer is dx / du, but still need to restrict this to the manifold latents\n",
    "inv_jacobian_outer = inv_jacobian_outer[:, :, : model.latent_dim]\n",
    "# And finally calculate log det (J^T J)\n",
    "jtj = torch.bmm(torch.transpose(inv_jacobian_outer, -2, -1), inv_jacobian_outer)\n",
    "mf_log_det_outer = - 0.5 * torch.slogdet(jtj)[1]\n",
    "\n",
    "log_prob_latent = model.manifold_latent_distribution._log_prob(u, context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,9))\n",
    "\n",
    "for panel, (label, quantity) in enumerate(zip(\n",
    "    [\"Perp latent\", \"Manifold latent after outer flow\", \"Manifold latent after inner flow\",\n",
    "     \"log det outer (MF)\", \"log det inner (MF)\", \"Base log prob\",\n",
    "     \"log det outer (PIE)\", \"log det outer (Slice of PIE)\", \"log det inner (Slice of PIE)\"],\n",
    "    [h_orthogonal, h_manifold, u,\n",
    "     mf_log_det_outer, - log_det_inner, log_prob_latent,\n",
    "     log_det_outer, - inv_log_det_outer, - inv_log_det_inner]\n",
    ")):\n",
    "    ax = plt.subplot(3,3,panel+1)\n",
    "    \n",
    "    quantity_ = quantity.detach().numpy()\n",
    "    quantity_ = quantity_.flatten() + np.zeros((res**2))\n",
    "    zmin, zmax = np.mean(quantity_) - 1.5 * np.std(quantity_), np.mean(quantity_) + 1.5 * np.std(quantity_)\n",
    "\n",
    "    pcm = plt.pcolormesh(\n",
    "        x_range, y_range,\n",
    "        np.clip(np.clip(quantity_, zmin, zmax).reshape(res, res), zmin, zmax),\n",
    "        cmap=\"viridis\", norm=matplotlib.colors.Normalize(zmin, zmax)\n",
    "    )\n",
    "    cb = plt.colorbar(pcm, extend=\"both\")\n",
    "    # plt.scatter(x_gen_mf[::10,0], x_gen_mf[::10,1], s=3., c=\"black\")\n",
    "\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    cb.set_label(label)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"spherical_gaussian_2d_anatomy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
