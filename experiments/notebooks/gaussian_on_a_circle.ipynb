{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy study: Gaussian on a circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from manifold_flow import transforms, utils, distributions, training\n",
    "from manifold_flow.flows import Flow, ManifoldFlow\n",
    "from manifold_flow import nn as nn_\n",
    "from experiments.simulators.spherical_simulator import SphericalGaussianSimulator\n",
    "from experiments.utils.models import create_vector_transform\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)-5.5s %(name)-30.30s %(levelname)-7.7s %(message)s\",\n",
    "    datefmt=\"%H:%M\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"experiments\" not in key and \"manifold_flow\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "n_train = 10000\n",
    "epsilon = 0.01\n",
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = SphericalGaussianSimulator(latent_dim=1, data_dim=2, epsilon=epsilon)\n",
    "x_sim = simulator.sample(n_train)\n",
    "x_sim_tensor = torch.from_numpy(x_sim)\n",
    "train_dataset = TensorDataset(x_sim_tensor, x_sim_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = 200\n",
    "# x_range = np.linspace(-1.4,1.4,res)\n",
    "# y_range = np.linspace(-1.4,1.4,res)\n",
    "# xx, yy = np.meshgrid(x_range, y_range)\n",
    "# x_grid = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "# logp_grid = simulator.log_density(x_grid, precise=True).reshape((res, res))\n",
    "# logp_grid[~np.isfinite(logp_grid)] = -1000000.\n",
    "\n",
    "# zmin, zmax = np.max(logp_grid) - 10, np.max(logp_grid)\n",
    "# fig = plt.figure(figsize=(5,4))\n",
    "# ax = plt.gca()\n",
    "\n",
    "# pcm = plt.imshow(\n",
    "#     np.clip(logp_grid, zmin, zmax),\n",
    "#     extent=(-1.4,1.4,-1.4,1.4),\n",
    "#     origin=\"lower\",\n",
    "#     cmap=\"viridis\", norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "#     interpolation='nearest'\n",
    "# )\n",
    "# cb = plt.colorbar(pcm, extend=\"both\")\n",
    "# plt.scatter(x_sim[::50,0], x_sim[::50,1], s=2., c=\"black\", alpha=1.)\n",
    "\n",
    "# plt.xlabel(\"$x_1$\")\n",
    "# plt.ylabel(\"$x_2$\")\n",
    "# cb.set_label(\"$\\log \\; p(x)$\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"../figures/simulator_toy.pdf\", dpi=500)\n",
    "\n",
    "# pixel_size = (x_range[1] - x_range[0]) * (y_range[1] - y_range[0])\n",
    "\n",
    "# print(\"Integral over density =\", np.sum(np.exp(logp_grid) * pixel_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = OrderedDict()\n",
    "labels[\"truth\"] = \"Truth\"\n",
    "labels[\"sf\"] = \"Standard flow\"\n",
    "labels[\"pie\"] = \"PIE (manifold)\"\n",
    "labels[\"pie_full\"] = \"PIE (all)\"\n",
    "labels[\"mf\"] = \"MF\"\n",
    "labels[\"mlfl\"] = \"MLF-L\"\n",
    "labels[\"mlfa\"] = \"MLF-A\"\n",
    "labels[\"mlfot\"] = \"MLF-OT\"\n",
    "labels[\"mlfae\"] = \"MLF-AE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = Flow(\n",
    "    data_dim=2,\n",
    "    transform=create_vector_transform(2, 10, base_transform_type=\"affine-coupling\"),\n",
    ")\n",
    "\n",
    "if train:\n",
    "    trainer = training.trainer.ManifoldFlowTrainer(sf)\n",
    "    trainer.train(\n",
    "        train_dataset,\n",
    "        [training.losses.nll],\n",
    "        loss_weights=[1.],\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    torch.save(sf.state_dict(), \"../data/models/sf_toy.pt\")\n",
    "else:\n",
    "    sf.load_state_dict(torch.load(\"../data/models/sf_toy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold flow (with specified manifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:12 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "16:12 manifold_flow.training.trainer INFO    Epoch   6: train loss  3.02396 (nll:  3.024)\n",
      "16:12 manifold_flow.training.trainer INFO               val. loss   2.94437 (nll:  2.944)\n",
      "16:12 manifold_flow.training.trainer INFO    Epoch  12: train loss  2.16272 (nll:  2.163)\n",
      "16:12 manifold_flow.training.trainer INFO               val. loss   2.12582 (nll:  2.126)\n",
      "16:12 manifold_flow.training.trainer INFO    Epoch  18: train loss  1.76621 (nll:  1.766)\n",
      "16:12 manifold_flow.training.trainer INFO               val. loss   1.75563 (nll:  1.756)\n",
      "16:12 manifold_flow.training.trainer INFO    Epoch  24: train loss  1.58474 (nll:  1.585)\n",
      "16:12 manifold_flow.training.trainer INFO               val. loss   1.58025 (nll:  1.580)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  30: train loss  1.47470 (nll:  1.475)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.47297 (nll:  1.473)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  36: train loss  1.39742 (nll:  1.397)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.39818 (nll:  1.398)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  42: train loss  1.34053 (nll:  1.341)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.34365 (nll:  1.344)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  48: train loss  1.29815 (nll:  1.298)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.30343 (nll:  1.303)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  54: train loss  1.26661 (nll:  1.267)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.27367 (nll:  1.274)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  60: train loss  1.24322 (nll:  1.243)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.25193 (nll:  1.252)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  66: train loss  1.22601 (nll:  1.226)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.23595 (nll:  1.236)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  72: train loss  1.21344 (nll:  1.213)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.22453 (nll:  1.225)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  78: train loss  1.20438 (nll:  1.204)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.21634 (nll:  1.216)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  84: train loss  1.19794 (nll:  1.198)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.21060 (nll:  1.211)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  90: train loss  1.19345 (nll:  1.193)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.20673 (nll:  1.207)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch  96: train loss  1.19043 (nll:  1.190)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.20409 (nll:  1.204)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch 102: train loss  1.18851 (nll:  1.189)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.20247 (nll:  1.202)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch 108: train loss  1.18743 (nll:  1.187)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.20157 (nll:  1.202)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch 114: train loss  1.18695 (nll:  1.187)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.20118 (nll:  1.201)\n",
      "16:13 manifold_flow.training.trainer INFO    Epoch 120: train loss  1.18684 (nll:  1.187)\n",
      "16:13 manifold_flow.training.trainer INFO               val. loss   1.20110 (nll:  1.201)\n",
      "16:13 manifold_flow.training.trainer INFO    Early stopping did not improve performance\n"
     ]
    }
   ],
   "source": [
    "mf = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    inner_transform=transforms.ConditionalAffineScalarTransform(features=1),\n",
    "    outer_transform=transforms.SphericalCoordinates(n=1, r0=1., azimuthal_offset=-0.5*np.pi)\n",
    ")\n",
    "\n",
    "if train:\n",
    "    trainer = training.trainer.ManifoldFlowTrainer(mf)\n",
    "    trainer.train(\n",
    "        train_dataset,\n",
    "        [training.losses.nll],\n",
    "        loss_weights=[1.],\n",
    "        epochs=epochs,\n",
    "        forward_kwargs={\"mode\":\"mf\"}\n",
    "    )\n",
    "    torch.save(mf.state_dict(), \"../data/models/mf_toy.pt\")\n",
    "else:\n",
    "    mf.load_state_dict(torch.load(\"../data/models/mf_toy.010.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    outer_transform=create_vector_transform(2, 5, base_transform_type=\"affine-coupling\"),\n",
    "    inner_transform=transforms.ConditionalAffineScalarTransform(features=1),\n",
    "    pie_epsilon=0.01,\n",
    ")\n",
    "\n",
    "if train:\n",
    "    trainer = training.trainer.ManifoldFlowTrainer(pie)\n",
    "    trainer.train(\n",
    "        train_dataset,\n",
    "        [training.losses.nll],\n",
    "        loss_weights=[1.],\n",
    "        epochs=epochs,\n",
    "        forward_kwargs={\"mode\":\"pie\"}\n",
    "    )\n",
    "    torch.save(pie.state_dict(), \"../data/models/pie_toy.pt\")\n",
    "else:\n",
    "    pie.load_state_dict(torch.load(\"../data/models/pie_toy.pt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLF-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlfl = ManifoldFlow(\n",
    "#     data_dim=2,\n",
    "#     latent_dim=1,\n",
    "#     inner_transform=transforms.ConditionalAffineScalarTransform(features=1),\n",
    "#     outer_transform=create_vector_transform(2, 3, base_transform_type=\"affine-coupling\")\n",
    "# )\n",
    "\n",
    "# if train:\n",
    "#     trainer = training.trainer.ManifoldFlowTrainer(mlfl)\n",
    "#     trainer.train(\n",
    "#         train_dataset,\n",
    "#         [training.losses.mse],\n",
    "#         loss_weights=[100.],\n",
    "#         epochs=epochs // 3,\n",
    "#         forward_kwargs={\"mode\":\"projection\"}\n",
    "#     )\n",
    "#     trainer.train(\n",
    "#         train_dataset,\n",
    "#         [training.losses.mse, training.losses.nll],\n",
    "#         loss_weights=[100., 0.1],\n",
    "#         epochs=epochs // 3,\n",
    "#         forward_kwargs={\"mode\":\"mf\"}\n",
    "#     )\n",
    "#     trainer.train(\n",
    "#         train_dataset,\n",
    "#         [training.losses.mse, training.losses.nll],\n",
    "#         loss_weights=[0., 1.],\n",
    "#         epochs=epochs // 3,\n",
    "#         parameters=mf.inner_transform.parameters(),\n",
    "#         forward_kwargs={\"mode\":\"mf\"}\n",
    "#     )\n",
    "#     torch.save(mlfl.state_dict(), \"../data/models/mlfl_toy.pt\")\n",
    "# else:\n",
    "#     mlfl.load_state_dict(torch.load(\"../data/models/mlfl_toy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLF-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:28 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "16:28 manifold_flow.training.trainer INFO    Training on CPU with single precision\n",
      "16:28 manifold_flow.training.trainer INFO    Epoch   6: train loss  0.56228 (MSE:  0.002, NLL:  3.818)\n",
      "16:28 manifold_flow.training.trainer INFO               val. loss   0.47213 (MSE:  0.001, NLL:  3.839)\n",
      "16:28 manifold_flow.training.trainer INFO    Epoch  12: train loss  0.48131 (MSE:  0.002, NLL:  2.680)\n",
      "16:28 manifold_flow.training.trainer INFO               val. loss   0.32946 (MSE:  0.001, NLL:  2.677)\n",
      "16:29 manifold_flow.training.trainer INFO    Epoch  18: train loss  0.31495 (MSE:  0.001, NLL:  2.019)\n",
      "16:29 manifold_flow.training.trainer INFO               val. loss   0.24908 (MSE:  0.000, NLL:  2.068)\n",
      "16:29 manifold_flow.training.trainer INFO    Epoch  24: train loss  0.22932 (MSE:  0.001, NLL:  1.636)\n",
      "16:29 manifold_flow.training.trainer INFO               val. loss   0.22231 (MSE:  0.000, NLL:  1.779)\n",
      "16:30 manifold_flow.training.trainer INFO    Epoch  30: train loss  0.31347 (MSE:  0.001, NLL:  2.114)\n",
      "16:30 manifold_flow.training.trainer INFO               val. loss   0.28544 (MSE:  0.001, NLL:  2.127)\n"
     ]
    }
   ],
   "source": [
    "mlfa = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    inner_transform=transforms.ConditionalAffineScalarTransform(features=1),\n",
    "    outer_transform=create_vector_transform(2, 5, base_transform_type=\"affine-coupling\")\n",
    ")\n",
    "\n",
    "if train:\n",
    "    trainer = training.ManifoldFlowTrainer(mlfa)\n",
    "    metatrainer = training.AlternatingTrainer(mlfa, trainer, trainer)\n",
    "    metatrainer.train(\n",
    "        train_dataset,\n",
    "        loss_functions=[training.losses.mse, training.losses.nll],\n",
    "        loss_function_trainers=[0, 1],\n",
    "        loss_labels=[\"MSE\", \"NLL\"],\n",
    "        loss_weights=[100., 0.1],\n",
    "        epochs=epochs,\n",
    "        parameters=[mlfa.parameters(), mlfa.inner_transform.parameters()],\n",
    "        trainer_kwargs=[{\"forward_kwargs\": {\"mode\": \"projection\"}}, {\"forward_kwargs\": {\"mode\": \"pie\"}}],\n",
    "    )\n",
    "    torch.save(mlfa.state_dict(), \"../data/models/mlfa_toy.pt\")\n",
    "else:\n",
    "    mlfa.load_state_dict(torch.load(\"../data/models/mlfa_toy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLF-OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlfot = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    inner_transform=transforms.ConditionalAffineScalarTransform(features=1),\n",
    "    outer_transform=create_vector_transform(2, 5, base_transform_type=\"affine-coupling\")\n",
    ")\n",
    "\n",
    "if train:\n",
    "    trainer = training.trainer.GenerativeTrainer(mlfot)\n",
    "    trainer.train(\n",
    "        train_dataset,\n",
    "        [training.losses.make_sinkhorn_divergence()],\n",
    "        loss_weights=[100.],\n",
    "        epochs=10*epochs,\n",
    "        batch_size=1000,\n",
    "    )\n",
    "    torch.save(mlfot.state_dict(), \"../data/models/mlfot_toy.pt\")\n",
    "else:\n",
    "    mlfot.load_state_dict(torch.load(\"../data/models/mlfot_toy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLF-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlfae = ManifoldFlow(\n",
    "    data_dim=2,\n",
    "    latent_dim=1,\n",
    "    inner_transform=transforms.ConditionalAffineScalarTransform(features=1),\n",
    "    outer_transform=create_vector_transform(2, 5, base_transform_type=\"affine-coupling\")\n",
    ")\n",
    "\n",
    "if train:\n",
    "    trainer = training.trainer.ManifoldFlowTrainer(mlfae)\n",
    "    trainer.train(\n",
    "        train_dataset,\n",
    "        [training.losses.mse],\n",
    "        loss_weights=[100.],\n",
    "        epochs=epochs,\n",
    "        forward_kwargs={\"mode\":\"projection\"}\n",
    "    )\n",
    "    torch.save(mlfae.state_dict(), \"../data/models/mlfae_toy.pt\")\n",
    "else:\n",
    "    mlfae.load_state_dict(torch.load(\"../data/models/mlfae_toy.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.eval()\n",
    "mf.eval()\n",
    "pie.eval()\n",
    "# mlfl.eval()\n",
    "mlfa.eval()\n",
    "mlfot.eval()\n",
    "mlfae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen = OrderedDict()\n",
    "\n",
    "x_gen[\"truth\"] = x_sim[:n]\n",
    "x_gen[\"sf\"] = sf.sample(n=1000).detach().numpy()\n",
    "x_gen[\"mf\"] = mf.sample(n=1000).detach().numpy()\n",
    "x_gen[\"pie\"] = pie.sample(n=1000).detach().numpy()\n",
    "x_gen[\"pie_full\"] = pie.sample(n=1000, sample_orthogonal=True).detach().numpy()\n",
    "# x_gen[\"mlfl\"] = mlfl.sample(n=1000).detach().numpy()\n",
    "x_gen[\"mlfa\"] = mlfa.sample(n=1000).detach().numpy()\n",
    "x_gen[\"mlfot\"] = mlfot.sample(n=1000).detach().numpy()\n",
    "x_gen[\"mlfae\"] = mlfae.sample(n=1000).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model likelihood over data space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 200\n",
    "boundary = 1.5\n",
    "im_extent = boundary + 0.5 * (2.*boundary/(res - 1))\n",
    "\n",
    "x_range = np.linspace(-boundary, boundary, res)\n",
    "y_range = np.linspace(-boundary, boundary, res)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "x_grid = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "x_grid_tensor = torch.FloatTensor(x_grid)\n",
    "\n",
    "\n",
    "logp_grid = OrderedDict()\n",
    "logp_grid[\"truth\"] = simulator.log_density(x_grid).reshape((res, res))\n",
    "logp_grid[\"truth\"][~np.isfinite(logp_grid[\"truth\"])] = -1000000.\n",
    "logp_grid[\"sf\"] = sf.log_prob(x_grid_tensor).detach().numpy().reshape((res, res))\n",
    "logp_grid[\"pie_full\"] = pie.log_prob(x_grid_tensor, mode=\"pie\").detach().numpy().reshape((res, res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_along_manifold(model, mode=\"mf\", zmin=-50., zmax=50., n_samples=1000, epsilon=0.02, max_insert=100):\n",
    "    # Sample\n",
    "    u = torch.linspace(zmin, zmax, n_samples).view(-1,1)\n",
    "    x = model.sample(n=n_samples, u=u).detach().numpy()\n",
    "    u = u.numpy().flatten()\n",
    "    \n",
    "    def in_box(x):\n",
    "        return np.abs(x[0]) < 1.5 or np.abs(x[1]) < 1.5\n",
    "    \n",
    "    # Interpolate\n",
    "    x_interpol = [x[0]]\n",
    "    for x_prev, x_now, u_prev, u_now  in zip(x[:-1], x[1:], u[:-1], u[1:]):\n",
    "        \n",
    "        # Check if we should generate more points in between\n",
    "        distance = np.linalg.norm(x_now-x_prev)\n",
    "        if distance > epsilon and (in_box(x_now) or in_box(x_prev)):\n",
    "            n_insert = min(int(distance / epsilon), max_insert)\n",
    "            u_insert = torch.linspace(u_prev, u_now, n_insert + 2)[1:-1].view(-1,1)\n",
    "            x_insert = model.sample(n=n_insert, u=u_insert).detach().numpy()\n",
    "            for x_ in x_insert:\n",
    "                if in_box(x_):\n",
    "                    x_interpol.append(x_)\n",
    "            \n",
    "        if in_box(x_now):\n",
    "            x_interpol.append(x_now)\n",
    "            \n",
    "    x_interpol = np.array(x_interpol)\n",
    "\n",
    "    # Evaluate likelihood\n",
    "    log_probs = model.log_prob(torch.FloatTensor(x_interpol), mode=mode).detach().numpy()\n",
    "\n",
    "    # Return\n",
    "    return x_interpol, log_probs\n",
    "\n",
    "\n",
    "logp_manifold, x_manifold = OrderedDict(), OrderedDict()\n",
    "x_manifold[\"mf\"], logp_manifold[\"mf\"] = likelihood_along_manifold(mf)\n",
    "# x_manifold[\"mlfl\"], logp_manifold[\"mlfl\"] = likelihood_along_manifold(mlfl)\n",
    "x_manifold[\"mlfa\"], logp_manifold[\"mlfa\"] = likelihood_along_manifold(mlfa)\n",
    "x_manifold[\"mlfot\"], logp_manifold[\"mlfot\"] = likelihood_along_manifold(mlfot)\n",
    "x_manifold[\"mlfae\"], logp_manifold[\"mlfae\"] = likelihood_along_manifold(mlfae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics for generated samples: mean distance from manifold, true likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_metrics(x, logp_min=-100., d_max=1., summary_fn=np.mean):\n",
    "    logp_gen = simulator.log_density(x)\n",
    "    logp_gen[(~np.isfinite(logp_gen)) + (logp_gen<logp_min)] = logp_min\n",
    "    logp_summary = summary_fn(logp_gen)\n",
    "    \n",
    "    d_gen = np.abs(np.sum(x**2, axis=1)**0.5 - 1)\n",
    "    d_gen[(~np.isfinite(d_gen)) + (d_gen>d_max)] = d_max\n",
    "    d_summary = summary_fn(d_gen)\n",
    "    \n",
    "    return logp_summary, d_summary\n",
    "\n",
    "\n",
    "logp_gen, d_gen = OrderedDict(), OrderedDict()\n",
    "\n",
    "for key, val in x_gen.items():\n",
    "    logp_gen[key], d_gen[key] = generative_metrics(x_gen[key])\n",
    "    \n",
    "\n",
    "print(\"Mean true log likelihood of samples generated from flows (higher is better):\")\n",
    "for key, val in logp_gen.items():\n",
    "    print(\"  {:>10.10s}: {:>6.1f}\".format(key, val))\n",
    "    \n",
    "print(\"Mean Euclidean distance between samples generated from flows and true manifold (lower is better):\")\n",
    "for key, val in d_gen.items():\n",
    "    print(\"  {:>10.10s}: {:>6.4f}\".format(key, val))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "show = [\"truth\", \"sf\", \"pie_full\", \"pie\", \"mf\", \"mlfa\", \"mlfot\", \"mlfae\"]\n",
    "\n",
    "fig = plt.figure(figsize=(4.*4,4.*2))\n",
    "\n",
    "for i, key in enumerate(show):\n",
    "    ax = plt.subplot(2,4,i+1)\n",
    "    \n",
    "    plt.scatter(x_gen[key][:n,0], x_gen[key][:n,1], s=10., c=\"C3\", label=labels[key])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim(-boundary, boundary)\n",
    "    plt.ylim(-boundary, boundary)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/generated_samples_toy.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = [\"truth\", \"sf\", \"pie_full\", \"mf\", \"mlfa\", \"mlfot\", \"mlfae\"]\n",
    "\n",
    "fig = plt.figure(figsize=(5.*5,4.*2))\n",
    "\n",
    "for i, key in enumerate(show):\n",
    "    ax = plt.subplot(2,4,i+1)\n",
    "    \n",
    "    try:\n",
    "        x = x_manifold[key]\n",
    "        logp = logp_manifold[key]\n",
    "        zmin, zmax = np.max(logp) - 5., np.max(logp)\n",
    "        \n",
    "        segments = np.concatenate([x[:-1,np.newaxis,:], x[1:,np.newaxis,:]], axis=1)\n",
    "        lc = LineCollection(segments, cmap='viridis', norm=plt.Normalize(zmin, zmax))\n",
    "        lc.set_array(np.clip(logp, zmin, zmax))\n",
    "        lc.set_linewidth(1.5)\n",
    "        im = ax.add_collection(lc)\n",
    "\n",
    "    except KeyError:\n",
    "        logp = logp_grid[key]\n",
    "        zmin, zmax = np.max(logp) - 5., np.max(logp)\n",
    "        im = plt.imshow(\n",
    "            np.clip(logp, zmin, zmax),\n",
    "            extent=(-im_extent, im_extent, -im_extent, im_extent),\n",
    "            origin=\"lower\",\n",
    "            cmap=\"viridis\",\n",
    "            norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "            interpolation='nearest'\n",
    "        )\n",
    "    cb = plt.colorbar(im, extend=\"both\")\n",
    "\n",
    "    plt.xlim(-im_extent, im_extent)\n",
    "    plt.ylim(-im_extent, im_extent)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    cb.set_label(r\"{} log likelihood\".format(labels[key]))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/log_likelihood_toy.pdf\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other plots (not updated to new conventions yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot reconstruction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_in = SphericalGaussianSimulator(1,2,epsilon=10*epsilon).sample(100)\n",
    "# x_in = torch.FloatTensor(x_in)\n",
    "# x_out = mf(x_in)[0]\n",
    "# x_in, x_out = x_in.detach().numpy(), x_out.detach().numpy()\n",
    "# dx = x_out - x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(5,5))\n",
    "# ax = plt.gca()\n",
    "# plt.scatter(x_in[:,0], x_in[:,1], s=15., c=\"C1\")\n",
    "# plt.scatter(x_out[:,0], x_out[:,1], s=15., c=\"C0\")\n",
    "# plt.quiver(\n",
    "#     x_in[:,0], x_in[:,1], dx[:,0], dx[:,1],\n",
    "#     angles='xy', scale_units='xy', scale=1., width=2.e-3, alpha=1.\n",
    "# )\n",
    "\n",
    "# plt.xlim(-1.5,1.5)\n",
    "# plt.ylim(-1.5,1.5)\n",
    "# plt.xlabel(\"$x_1$\")\n",
    "# plt.ylabel(\"$x_2$\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"../figures/spherical_gaussian_2d_mf_reco.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_anatomy_plot(model, name, res=25):\n",
    "#     x_range = np.linspace(-1.5,1.5,res)\n",
    "#     y_range = np.linspace(-1.5,1.5,res)\n",
    "#     xx, yy = np.meshgrid(x_range, y_range)\n",
    "#     x = np.concatenate((xx.reshape((-1,1)), yy.reshape((-1,1))), axis=1)\n",
    "#     x = torch.FloatTensor(x)\n",
    "#     x.requires_grad = True\n",
    "\n",
    "#     # Encode\n",
    "#     u, h_manifold, h_orthogonal, log_det_outer, log_det_inner = model._encode(x)\n",
    "\n",
    "#     # Decode\n",
    "#     x_reco, _, _, inv_jacobian_outer = model._decode(u, mode=\"mf\")\n",
    "#     _, inv_log_det_inner, inv_log_det_outer, _ = model._decode(u, mode=\"slice\")\n",
    "\n",
    "#     # inv_jacobian_outer is dx / du, but still need to restrict this to the manifold latents\n",
    "#     inv_jacobian_outer = inv_jacobian_outer[:, :, : model.latent_dim]\n",
    "#     # And finally calculate log det (J^T J)\n",
    "#     jtj = torch.bmm(torch.transpose(inv_jacobian_outer, -2, -1), inv_jacobian_outer)\n",
    "#     mf_log_det_outer = - 0.5 * torch.slogdet(jtj)[1]\n",
    "\n",
    "#     # Base log prob\n",
    "#     log_prob_latent = model.manifold_latent_distribution._log_prob(u, context=None)\n",
    "    \n",
    "#     # Plot\n",
    "#     fig = plt.figure(figsize=(12,9))\n",
    "\n",
    "#     for panel, (label, quantity, diverging) in enumerate(zip(\n",
    "#         [\"Perp latent\", \"Manifold latent after outer flow\", \"Manifold latent after inner flow\",\n",
    "#          \"log det outer (PIE)\", \"log det inner\", \"Base log prob\",\n",
    "#          \"log det outer (MF)\", \"log det outer (Slice of PIE)\", \"delta log det outer (MF - Slice)\"],\n",
    "#         [h_orthogonal, h_manifold, u,\n",
    "#          log_det_outer, - log_det_inner, log_prob_latent,\n",
    "#          mf_log_det_outer, - inv_log_det_outer, mf_log_det_outer + inv_log_det_outer],\n",
    "#         [True, True, True, False, False, False, False, False, False]\n",
    "#     )):\n",
    "#         ax = plt.subplot(3,3,panel+1)\n",
    "\n",
    "#         quantity_ = quantity.detach().numpy()\n",
    "#         quantity_ = quantity_.flatten() + np.zeros((res**2))\n",
    "\n",
    "#         if diverging:\n",
    "#             zmin, zmax = - 2. * np.std(quantity_), 2. * np.std(quantity_)\n",
    "#         else:\n",
    "#             zmin, zmax = np.mean(quantity_) - 1.5 * np.std(quantity_), np.mean(quantity_) + 1.5 * np.std(quantity_)\n",
    "\n",
    "#         pcm = plt.imshow(\n",
    "#             np.clip(quantity_, zmin, zmax).reshape(res, res),\n",
    "#             extent=(-1.5, 1.5, -1.5, 1.5),\n",
    "#             origin=\"lower\",\n",
    "#             cmap=\"PRGn\" if diverging else \"viridis\",\n",
    "#             norm=matplotlib.colors.Normalize(zmin, zmax),\n",
    "#             interpolation='nearest'\n",
    "#         )\n",
    "#         cb = plt.colorbar(pcm, extend=\"both\")\n",
    "#         # plt.scatter(x_gen_mf[::10,0], x_gen_mf[::10,1], s=3., c=\"black\")\n",
    "\n",
    "#         plt.xlim(-1.5,1.5)\n",
    "#         plt.ylim(-1.5,1.5)\n",
    "#         plt.xlabel(\"$x_1$\")\n",
    "#         plt.ylabel(\"$x_2$\")\n",
    "#         cb.set_label(label)\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#     plt.savefig(\"../figures/spherical_gaussian_2d_{}_anatomy.pdf\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
